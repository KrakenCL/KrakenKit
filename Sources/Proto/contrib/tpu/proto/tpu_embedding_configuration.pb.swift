// DO NOT EDIT.
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: tensorflow/contrib/tpu/proto/tpu_embedding_configuration.proto
//
// For information on using the generated types, please see the documenation:
//   https://github.com/apple/swift-protobuf/

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that your are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

public struct Tensorflow_Tpu_TPUEmbeddingConfiguration {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  public var tableDescriptor: [Tensorflow_Tpu_TPUEmbeddingConfiguration.TableDescriptor] {
    get {return _storage._tableDescriptor}
    set {_uniqueStorage()._tableDescriptor = newValue}
  }

  public var mode: Tensorflow_Tpu_TPUEmbeddingConfiguration.Mode {
    get {return _storage._mode}
    set {_uniqueStorage()._mode = newValue}
  }

  /// Number of samples in each batch of embedding layer activations sent to
  /// the TensorCore.
  public var batchSizePerTensorCore: Int32 {
    get {return _storage._batchSizePerTensorCore}
    set {_uniqueStorage()._batchSizePerTensorCore = newValue}
  }

  /// Number of TPU hosts used for inference/training.
  public var numHosts: Int32 {
    get {return _storage._numHosts}
    set {_uniqueStorage()._numHosts = newValue}
  }

  /// Number of TensorCore used for inference/training.
  public var numTensorCores: Int32 {
    get {return _storage._numTensorCores}
    set {_uniqueStorage()._numTensorCores = newValue}
  }

  public var shardingStrategy: Tensorflow_Tpu_TPUEmbeddingConfiguration.ShardingStrategy {
    get {return _storage._shardingStrategy}
    set {_uniqueStorage()._shardingStrategy = newValue}
  }

  /// This parameter determines if the execution of the sparse core will be
  /// pipelined with that of the TensorCore. This parameter only affects results
  /// when mode=TRAINING. If mode=INFERENCE or BACKWARD_PASS_ONLY, this parameter
  /// does not affect execution and hence, is a don't care value.
  ///
  /// false: The execution of the sparse core is not pipelined with that of the
  /// TensorCore. The forward pass of every step on the sparse core is executed
  /// only after the backward pass of the previous step is complete. And the
  /// backward pass on the sparse core is executed only after the embedding
  /// gradients have been computed on the TensorCore on every step. This ensures
  /// that the activations on every step observe the gradient updates from the
  /// previous step on both the sparse core and the TensorCore.
  ///
  /// true: The execution of the sparse core is pipelined with that of the
  /// TensorCore. The forward pass of every step on the sparse core can be
  /// executed after the forward pass of the previous step is complete without
  /// waiting for the backward pass. This improves the utilization of the sparse
  /// core allowing it to process step N+1 while the embedding gradients for step
  /// N are computed on the TensorCore. The backward pass of every step on the
  /// sparse core is executed directly after the forward pass for the next step
  /// is complete. The drawback is that embedding activations for step N+1 do not
  /// observe the embedding gradient updates from step N. This could affect model
  /// quality if step N and N+1 involve the same set of embedding IDs. However,
  /// since the embedding updates are sparse, this is generally not considered a
  /// problem.
  public var pipelineExecutionWithTensorCore: Bool {
    get {return _storage._pipelineExecutionWithTensorCore}
    set {_uniqueStorage()._pipelineExecutionWithTensorCore = newValue}
  }

  /// Extended output layout information; if not provided, a compatibility mode
  /// will use defaults that match the old layout. Providing a value for this
  /// field is EXPERIMENTAL and most ways of filling it will probably break. Do
  /// not set it unless you know what you are doing.
  public var outputLayout: Tensorflow_Tpu_TPUEmbeddingOutputLayout {
    get {return _storage._outputLayout ?? Tensorflow_Tpu_TPUEmbeddingOutputLayout()}
    set {_uniqueStorage()._outputLayout = newValue}
  }
  /// Returns true if `outputLayout` has been explicitly set.
  public var hasOutputLayout: Bool {return _storage._outputLayout != nil}
  /// Clears the value of `outputLayout`. Subsequent reads from it will return its default value.
  public mutating func clearOutputLayout() {_uniqueStorage()._outputLayout = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Mode. Should the embedding layer program be run for inference (just forward
  /// pass), training (both forward and backward pass) or just the backward_pass.
  public enum Mode: SwiftProtobuf.Enum {
    public typealias RawValue = Int
    case unspecified // = 0
    case inference // = 1
    case training // = 2
    case backwardPassOnly // = 3
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .inference
      case 2: self = .training
      case 3: self = .backwardPassOnly
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .inference: return 1
      case .training: return 2
      case .backwardPassOnly: return 3
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  /// Sharding strategy of the embedding tables among the hosts.
  /// If the sharding_strategy is "mod", each id is assigned to host
  /// "id % num_hosts". For instance, 13 ids are split across 5 hosts as:
  /// [[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]].
  /// If the sharding_strategy is "div", ids are assigned to hosts in a
  /// contiguous manner. In this case, 13 ids are split across 5 hosts as:
  /// [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]].
  /// In both the strategies, if the id space does not evenly divide the number
  /// of hosts, each of the first "table_descriptor.num_ids % num_hosts" hosts
  /// will be assigned one more id.
  /// This partitioning strategy exactly follows that in the embedding_lookup
  /// TensorFlow function at tensorflow/python/ops/embedding_ops.py.
  public enum ShardingStrategy: SwiftProtobuf.Enum {
    public typealias RawValue = Int
    case divDefault // = 0
    case mod // = 1
    case UNRECOGNIZED(Int)

    public init() {
      self = .divDefault
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .divDefault
      case 1: self = .mod
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .divDefault: return 0
      case .mod: return 1
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  /// Description of the various embedding tables.
  public struct TableDescriptor {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Name of the table.
    public var name: String {
      get {return _storage._name}
      set {_uniqueStorage()._name = newValue}
    }

    /// Size of the vocabulary (i.e., number of rows) in the table.
    public var vocabularySize: Int32 {
      get {return _storage._vocabularySize}
      set {_uniqueStorage()._vocabularySize = newValue}
    }

    /// The embedding dimension (i.e., the width of the embedding table).
    public var dimension: Int32 {
      get {return _storage._dimension}
      set {_uniqueStorage()._dimension = newValue}
    }

    /// Number of features mapped to this table.
    public var numFeatures: Int32 {
      get {return _storage._numFeatures}
      set {_uniqueStorage()._numFeatures = newValue}
    }

    /// Details of the learning algorithm used to update the embedding
    /// parameters.
    public var optimizationParameters: Tensorflow_Tpu_OptimizationParameters {
      get {return _storage._optimizationParameters ?? Tensorflow_Tpu_OptimizationParameters()}
      set {_uniqueStorage()._optimizationParameters = newValue}
    }
    /// Returns true if `optimizationParameters` has been explicitly set.
    public var hasOptimizationParameters: Bool {return _storage._optimizationParameters != nil}
    /// Clears the value of `optimizationParameters`. Subsequent reads from it will return its default value.
    public mutating func clearOptimizationParameters() {_uniqueStorage()._optimizationParameters = nil}

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    public init() {}

    fileprivate var _storage = _StorageClass.defaultInstance
  }

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

#if swift(>=4.2)

extension Tensorflow_Tpu_TPUEmbeddingConfiguration.Mode: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Tensorflow_Tpu_TPUEmbeddingConfiguration.Mode] = [
    .unspecified,
    .inference,
    .training,
    .backwardPassOnly,
  ]
}

extension Tensorflow_Tpu_TPUEmbeddingConfiguration.ShardingStrategy: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Tensorflow_Tpu_TPUEmbeddingConfiguration.ShardingStrategy] = [
    .divDefault,
    .mod,
  ]
}

#endif  // swift(>=4.2)

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "tensorflow.tpu"

extension Tensorflow_Tpu_TPUEmbeddingConfiguration: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TPUEmbeddingConfiguration"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "table_descriptor"),
    2: .same(proto: "mode"),
    3: .standard(proto: "batch_size_per_tensor_core"),
    4: .standard(proto: "num_hosts"),
    5: .standard(proto: "num_tensor_cores"),
    6: .standard(proto: "sharding_strategy"),
    7: .standard(proto: "pipeline_execution_with_tensor_core"),
    8: .standard(proto: "output_layout"),
  ]

  fileprivate class _StorageClass {
    var _tableDescriptor: [Tensorflow_Tpu_TPUEmbeddingConfiguration.TableDescriptor] = []
    var _mode: Tensorflow_Tpu_TPUEmbeddingConfiguration.Mode = .unspecified
    var _batchSizePerTensorCore: Int32 = 0
    var _numHosts: Int32 = 0
    var _numTensorCores: Int32 = 0
    var _shardingStrategy: Tensorflow_Tpu_TPUEmbeddingConfiguration.ShardingStrategy = .divDefault
    var _pipelineExecutionWithTensorCore: Bool = false
    var _outputLayout: Tensorflow_Tpu_TPUEmbeddingOutputLayout? = nil

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _tableDescriptor = source._tableDescriptor
      _mode = source._mode
      _batchSizePerTensorCore = source._batchSizePerTensorCore
      _numHosts = source._numHosts
      _numTensorCores = source._numTensorCores
      _shardingStrategy = source._shardingStrategy
      _pipelineExecutionWithTensorCore = source._pipelineExecutionWithTensorCore
      _outputLayout = source._outputLayout
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1: try decoder.decodeRepeatedMessageField(value: &_storage._tableDescriptor)
        case 2: try decoder.decodeSingularEnumField(value: &_storage._mode)
        case 3: try decoder.decodeSingularInt32Field(value: &_storage._batchSizePerTensorCore)
        case 4: try decoder.decodeSingularInt32Field(value: &_storage._numHosts)
        case 5: try decoder.decodeSingularInt32Field(value: &_storage._numTensorCores)
        case 6: try decoder.decodeSingularEnumField(value: &_storage._shardingStrategy)
        case 7: try decoder.decodeSingularBoolField(value: &_storage._pipelineExecutionWithTensorCore)
        case 8: try decoder.decodeSingularMessageField(value: &_storage._outputLayout)
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if !_storage._tableDescriptor.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._tableDescriptor, fieldNumber: 1)
      }
      if _storage._mode != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._mode, fieldNumber: 2)
      }
      if _storage._batchSizePerTensorCore != 0 {
        try visitor.visitSingularInt32Field(value: _storage._batchSizePerTensorCore, fieldNumber: 3)
      }
      if _storage._numHosts != 0 {
        try visitor.visitSingularInt32Field(value: _storage._numHosts, fieldNumber: 4)
      }
      if _storage._numTensorCores != 0 {
        try visitor.visitSingularInt32Field(value: _storage._numTensorCores, fieldNumber: 5)
      }
      if _storage._shardingStrategy != .divDefault {
        try visitor.visitSingularEnumField(value: _storage._shardingStrategy, fieldNumber: 6)
      }
      if _storage._pipelineExecutionWithTensorCore != false {
        try visitor.visitSingularBoolField(value: _storage._pipelineExecutionWithTensorCore, fieldNumber: 7)
      }
      if let v = _storage._outputLayout {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Tensorflow_Tpu_TPUEmbeddingConfiguration, rhs: Tensorflow_Tpu_TPUEmbeddingConfiguration) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._tableDescriptor != rhs_storage._tableDescriptor {return false}
        if _storage._mode != rhs_storage._mode {return false}
        if _storage._batchSizePerTensorCore != rhs_storage._batchSizePerTensorCore {return false}
        if _storage._numHosts != rhs_storage._numHosts {return false}
        if _storage._numTensorCores != rhs_storage._numTensorCores {return false}
        if _storage._shardingStrategy != rhs_storage._shardingStrategy {return false}
        if _storage._pipelineExecutionWithTensorCore != rhs_storage._pipelineExecutionWithTensorCore {return false}
        if _storage._outputLayout != rhs_storage._outputLayout {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorflow_Tpu_TPUEmbeddingConfiguration.Mode: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "UNSPECIFIED"),
    1: .same(proto: "INFERENCE"),
    2: .same(proto: "TRAINING"),
    3: .same(proto: "BACKWARD_PASS_ONLY"),
  ]
}

extension Tensorflow_Tpu_TPUEmbeddingConfiguration.ShardingStrategy: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "DIV_DEFAULT"),
    1: .same(proto: "MOD"),
  ]
}

extension Tensorflow_Tpu_TPUEmbeddingConfiguration.TableDescriptor: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Tensorflow_Tpu_TPUEmbeddingConfiguration.protoMessageName + ".TableDescriptor"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .standard(proto: "vocabulary_size"),
    3: .same(proto: "dimension"),
    4: .standard(proto: "num_features"),
    5: .standard(proto: "optimization_parameters"),
  ]

  fileprivate class _StorageClass {
    var _name: String = String()
    var _vocabularySize: Int32 = 0
    var _dimension: Int32 = 0
    var _numFeatures: Int32 = 0
    var _optimizationParameters: Tensorflow_Tpu_OptimizationParameters? = nil

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _name = source._name
      _vocabularySize = source._vocabularySize
      _dimension = source._dimension
      _numFeatures = source._numFeatures
      _optimizationParameters = source._optimizationParameters
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1: try decoder.decodeSingularStringField(value: &_storage._name)
        case 2: try decoder.decodeSingularInt32Field(value: &_storage._vocabularySize)
        case 3: try decoder.decodeSingularInt32Field(value: &_storage._dimension)
        case 4: try decoder.decodeSingularInt32Field(value: &_storage._numFeatures)
        case 5: try decoder.decodeSingularMessageField(value: &_storage._optimizationParameters)
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if !_storage._name.isEmpty {
        try visitor.visitSingularStringField(value: _storage._name, fieldNumber: 1)
      }
      if _storage._vocabularySize != 0 {
        try visitor.visitSingularInt32Field(value: _storage._vocabularySize, fieldNumber: 2)
      }
      if _storage._dimension != 0 {
        try visitor.visitSingularInt32Field(value: _storage._dimension, fieldNumber: 3)
      }
      if _storage._numFeatures != 0 {
        try visitor.visitSingularInt32Field(value: _storage._numFeatures, fieldNumber: 4)
      }
      if let v = _storage._optimizationParameters {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Tensorflow_Tpu_TPUEmbeddingConfiguration.TableDescriptor, rhs: Tensorflow_Tpu_TPUEmbeddingConfiguration.TableDescriptor) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._name != rhs_storage._name {return false}
        if _storage._vocabularySize != rhs_storage._vocabularySize {return false}
        if _storage._dimension != rhs_storage._dimension {return false}
        if _storage._numFeatures != rhs_storage._numFeatures {return false}
        if _storage._optimizationParameters != rhs_storage._optimizationParameters {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
